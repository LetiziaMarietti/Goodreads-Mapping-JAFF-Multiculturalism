{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b484d0",
   "metadata": {
    "id": "79b484d0"
   },
   "source": [
    "# Crawl Goodreads Book Pages \n",
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0356995",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882914dc-20c5-4805-9729-53fadb1ddcc3",
   "metadata": {},
   "source": [
    "## Import dataset obtained by scraping books metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4265e41-5918-47ab-9d79-2428a6ac911d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author ID</th>\n",
       "      <th>Author name</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Place of birth</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Genres</th>\n",
       "      <th>URL_authors</th>\n",
       "      <th>title</th>\n",
       "      <th>URL_books</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>Number of works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1332.0</td>\n",
       "      <td>Julia Golding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Website</td>\n",
       "      <td>My journey to becoming an author has been a ro...</td>\n",
       "      <td>Young Adult; ,; Children's</td>\n",
       "      <td>https://www.goodreads.com/author/show/1332.Jul...</td>\n",
       "      <td>Les Enqu√™tes de Jane Austen - tome 2 - Un vol...</td>\n",
       "      <td>https://www.goodreads.com/book/show/222068206-...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1332.0</td>\n",
       "      <td>Julia Golding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Website</td>\n",
       "      <td>My journey to becoming an author has been a ro...</td>\n",
       "      <td>Young Adult; ,; Children's</td>\n",
       "      <td>https://www.goodreads.com/author/show/1332.Jul...</td>\n",
       "      <td>The Austen Intrigue (Regency Secrets #4)</td>\n",
       "      <td>https://www.goodreads.com/book/show/232290671-...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1332.0</td>\n",
       "      <td>Julia Golding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Website</td>\n",
       "      <td>My journey to becoming an author has been a ro...</td>\n",
       "      <td>Young Adult; ,; Children's</td>\n",
       "      <td>https://www.goodreads.com/author/show/1332.Jul...</td>\n",
       "      <td>Les enqu√™tes de Jane Austen - Tome 2: Un vole...</td>\n",
       "      <td>https://www.goodreads.com/book/show/220520725-...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1332.0</td>\n",
       "      <td>Julia Golding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Website</td>\n",
       "      <td>My journey to becoming an author has been a ro...</td>\n",
       "      <td>Young Adult; ,; Children's</td>\n",
       "      <td>https://www.goodreads.com/author/show/1332.Jul...</td>\n",
       "      <td>Jane Austen Investigates: The Abbey Mystery (J...</td>\n",
       "      <td>https://www.goodreads.com/book/show/56933218-j...</td>\n",
       "      <td>3.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1332.0</td>\n",
       "      <td>Julia Golding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Website</td>\n",
       "      <td>My journey to becoming an author has been a ro...</td>\n",
       "      <td>Young Adult; ,; Children's</td>\n",
       "      <td>https://www.goodreads.com/author/show/1332.Jul...</td>\n",
       "      <td>The Burglar's Ball (Jane Austen Investigates, #2)</td>\n",
       "      <td>https://www.goodreads.com/book/show/58445472-t...</td>\n",
       "      <td>4.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Author ID    Author name Date of birth Place of birth  \\\n",
       "0     1332.0  Julia Golding           NaN        Website   \n",
       "1     1332.0  Julia Golding           NaN        Website   \n",
       "2     1332.0  Julia Golding           NaN        Website   \n",
       "3     1332.0  Julia Golding           NaN        Website   \n",
       "4     1332.0  Julia Golding           NaN        Website   \n",
       "\n",
       "                                           Biography  \\\n",
       "0  My journey to becoming an author has been a ro...   \n",
       "1  My journey to becoming an author has been a ro...   \n",
       "2  My journey to becoming an author has been a ro...   \n",
       "3  My journey to becoming an author has been a ro...   \n",
       "4  My journey to becoming an author has been a ro...   \n",
       "\n",
       "                       Genres  \\\n",
       "0  Young Adult; ,; Children's   \n",
       "1  Young Adult; ,; Children's   \n",
       "2  Young Adult; ,; Children's   \n",
       "3  Young Adult; ,; Children's   \n",
       "4  Young Adult; ,; Children's   \n",
       "\n",
       "                                         URL_authors  \\\n",
       "0  https://www.goodreads.com/author/show/1332.Jul...   \n",
       "1  https://www.goodreads.com/author/show/1332.Jul...   \n",
       "2  https://www.goodreads.com/author/show/1332.Jul...   \n",
       "3  https://www.goodreads.com/author/show/1332.Jul...   \n",
       "4  https://www.goodreads.com/author/show/1332.Jul...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Les Enqu√™tes de Jane Austen - tome 2 - Un vol...   \n",
       "1           The Austen Intrigue (Regency Secrets #4)   \n",
       "2  Les enqu√™tes de Jane Austen - Tome 2: Un vole...   \n",
       "3  Jane Austen Investigates: The Abbey Mystery (J...   \n",
       "4  The Burglar's Ball (Jane Austen Investigates, #2)   \n",
       "\n",
       "                                           URL_books  average_rating  \\\n",
       "0  https://www.goodreads.com/book/show/222068206-...            0.00   \n",
       "1  https://www.goodreads.com/book/show/232290671-...            0.00   \n",
       "2  https://www.goodreads.com/book/show/220520725-...            0.00   \n",
       "3  https://www.goodreads.com/book/show/56933218-j...            3.98   \n",
       "4  https://www.goodreads.com/book/show/58445472-t...            4.11   \n",
       "\n",
       "   year_of_publication  Number of works  \n",
       "0                  NaN             85.0  \n",
       "1                  NaN             85.0  \n",
       "2                  NaN             85.0  \n",
       "3                  NaN             85.0  \n",
       "4                  NaN             85.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'combined_JAFF_authors_grouped.csv'\n",
    "\n",
    "# The output directory\n",
    "html_dir = 'reviews/'\n",
    "\n",
    "# Read the core dataset as a pandas DataFrame and show the Goodreads link column\n",
    "source_df = pd.read_csv(file) \n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b7349",
   "metadata": {},
   "source": [
    "## Generate URLs for the page with info about all the editions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49945b4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/8226: https://www.goodreads.com/book/show/222068206-les-enqu-tes-de-jane-austen---tome-2---un-voleur-au-bal\n",
      "Processing 2/8226: https://www.goodreads.com/book/show/232290671-the-austen-intrigue\n",
      "Processing 3/8226: https://www.goodreads.com/book/show/220520725-les-enqu-tes-de-jane-austen---tome-2\n",
      "Processing 4/8226: https://www.goodreads.com/book/show/56933218-jane-austen-investigates\n",
      "Processing 5/8226: https://www.goodreads.com/book/show/58445472-the-burglar-s-ball\n",
      "Processing 6/8226: https://www.goodreads.com/book/show/59880929-jane-austen-investigates\n",
      "Processing 7/8226: https://www.goodreads.com/book/show/2152.The_Jane_Austen_Book_Club\n",
      "Processing 8/8226: https://www.goodreads.com/book/show/18914877-the-complete-novels\n",
      "Processing 9/8226: https://www.goodreads.com/book/show/136750299-persuasions\n",
      "Processing 10/8226: https://www.goodreads.com/book/show/43432578-the-essential-guide-to-jane-austen\n",
      "Processing 11/8226: https://www.goodreads.com/book/show/18300267-pride-and-prejudice\n",
      "Processing 12/8226: https://www.goodreads.com/book/show/23395733-emma\n",
      "Processing 13/8226: https://www.goodreads.com/book/show/23236641-emma\n",
      "Processing 14/8226: https://www.goodreads.com/book/show/50859122-love-islands\n",
      "Processing 15/8226: https://www.goodreads.com/book/show/29369578-the-ultimate-persuasion\n",
      "Processing 16/8226: https://www.goodreads.com/book/show/216949057-orgulho-e-tenta-o---aventura-a-dois-mnibus-geral-livro-150\n",
      "Processing 17/8226: https://www.goodreads.com/book/show/53320794-iludidas-pelo-orgulho-harlequin-jessica-livro-208\n",
      "Processing 18/8226: https://www.goodreads.com/book/show/43728346-orgulho-e-tenta-o-sabrina-livro-1387\n",
      "Processing 19/8226: https://www.goodreads.com/book/show/33395629-the-ultimate-persuasion-a-tempestuous-temptation-the-notorious-gabriel-d\n",
      "Processing 20/8226: https://www.goodreads.com/book/show/22518405-northanger-abbey\n",
      "Processing 21/8226: https://www.goodreads.com/book/show/18778806-northanger-abbey\n",
      "Processing 22/8226: https://www.goodreads.com/book/show/21424798-northanger-abbey\n",
      "Processing 23/8226: https://www.goodreads.com/book/show/10239347-pride-and-prejudice\n",
      "Processing 24/8226: https://www.goodreads.com/book/show/25852870-eligible\n",
      "Processing 25/8226: https://www.goodreads.com/book/show/14917.Desire_and_Duty\n",
      "Processing 26/8226: https://www.goodreads.com/book/show/8732153-pride-and-prejudice\n",
      "Processing 27/8226: https://www.goodreads.com/book/show/29410110-magic-and-manners\n",
      "Processing 28/8226: https://www.goodreads.com/book/show/10285809-jane-austen-made-me-do-it\n",
      "Processing 29/8226: https://www.goodreads.com/book/show/227828301-ladies-in-waiting\n",
      "Processing 30/8226: https://www.goodreads.com/book/show/9609078-the-strange-marriage-of-anne-de-bourgh\n",
      "Processing 31/8226: https://www.goodreads.com/book/show/21612404-the-strange-marriage-of-anne-de-bourgh\n",
      "Processing 32/8226: https://www.goodreads.com/book/show/14912.Conviction\n",
      "Processing 33/8226: https://www.goodreads.com/book/show/140680682-by-jane-dawkins---letters-from-pemberley\n",
      "Processing 34/8226: https://www.goodreads.com/book/show/201093373-letters-from-pemberley-the-first-year\n",
      "Processing 35/8226: https://www.goodreads.com/book/show/410261.Letters_from_Pemberley\n",
      "Processing 36/8226: https://www.goodreads.com/book/show/14916.More_Letters_from_Pemberley\n"
     ]
    }
   ],
   "source": [
    "def sleep():\n",
    "    time.sleep(1 + random.random())\n",
    "\n",
    "# Placeholder to store the generated URLs\n",
    "generated_urls = [] \n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36'\n",
    "}\n",
    "save_every = 50  # Save progress every 50 rows\n",
    "output_file = \"goodreads_editions_progress.csv\"\n",
    "total = len(source_df['URL_books'])\n",
    "\n",
    "for i, url in enumerate(source_df['URL_books']):\n",
    "    print(f\"Processing {i+1}/{total}: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        sleep()\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            pattern = r'https://www\\.goodreads\\.com/work/editions/\\d+'\n",
    "            matches = re.findall(pattern, html)\n",
    "            if matches:\n",
    "                generated_urls.append(matches[0])\n",
    "            else:\n",
    "                generated_urls.append(\"Fail\")\n",
    "        else:\n",
    "            generated_urls.append(\"Fail\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i+1}: {e}\")\n",
    "        generated_urls.append(\"Fail\")\n",
    "\n",
    "    # Save progress every N rows \n",
    "    if (i + 1) % save_every == 0:\n",
    "        source_df.loc[:i, 'Generated_URL'] = generated_urls[:i+1]\n",
    "        source_df.iloc[:i+1].to_csv(output_file, index=False)\n",
    "        print(f\"Progress saved at row {i+1}\")\n",
    "\n",
    "\n",
    "source_df['Generated_URL'] = generated_urls\n",
    "source_df.to_csv(\"goodreads_editions_final_grouped.csv\", index=False)\n",
    "print(\"Scraping completed and saved to goodreads_editions_final.csv\")\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3465d4f5-6667-48ef-8a49-d112ef67298f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "file_2 = 'goodreads_editions_final_grouped.csv'\n",
    "\n",
    "df_2 = pd.read_csv(file_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c3c24d-8326-4d08-b8bd-0ffdb8c5c469",
   "metadata": {},
   "source": [
    "Open file containing authors missing in file_2 (from authors who were missed or excluded during the initial filtering \n",
    "of JAFF books from the total dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc4d8a-3b92-47a5-8f45-e7d04a00891a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "file_3 = 'missing_books_generated_links.csv'\n",
    "\n",
    "df_3 = pd.read_csv(file_3)\n",
    "\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7e036-3535-4644-81c8-77e2c029984c",
   "metadata": {},
   "source": [
    "Make df_3 columns the same order as df_2 and concatenate the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978bec02-07bf-4343-ba5d-5756b8c016a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_3 = df_3[df_2.columns]\n",
    "\n",
    "df_concat = pd.concat([df_2, df_3], ignore_index=True)\n",
    "df_concat.to_csv('ready_to_scrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975d0a0-c3a1-4f03-a21b-56231f5f0b4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "file_4 = 'ready_to_scrape.csv'\n",
    "\n",
    "df_4 = pd.read_csv(file_4)\n",
    "\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e646db-c51a-4105-b657-82dce71209c5",
   "metadata": {},
   "source": [
    "## Extract metadata at edition level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304926f-25ac-4874-aa57-3b5d00627d8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sleep():\n",
    "    sleep_time = 1 + random.random() # decrease it to 1 sec for faster scraping \n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = df_4['Generated_URL'].tolist()\n",
    "total = len(urls)\n",
    "\n",
    "# Initialize lists to store each field for the DataFrame\n",
    "book_titles = []\n",
    "publication_dates = []\n",
    "publishers = []\n",
    "formats = []\n",
    "page_counts = []\n",
    "authors = []\n",
    "author_ids = []\n",
    "isbns = []\n",
    "asins = []\n",
    "languages = []\n",
    "ratings = []\n",
    "rating_counts = []\n",
    "book_urls = []\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36'\n",
    "}\n",
    "\n",
    "save_every = 50  # Save progress every 50 URLs\n",
    "output_file = \"goodreads_editions_data_progress.csv\"\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    print(f\"Processing work {i+1}/{total}: {url}\")\n",
    "    # Remove existing ?page= to safely add our custom page param\n",
    "    base_url = re.sub(r'\\?page=\\d+', '', url)\n",
    "    page = 1\n",
    "\n",
    "    while True:  # loop over all pages of editions!\n",
    "        page_url = f\"{base_url}?page={page}\"\n",
    "        try:\n",
    "            response = requests.get(page_url, headers=headers)\n",
    "            sleep()\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                book_elements = soup.find_all(\"div\", class_=\"elementList clearFix\")\n",
    "\n",
    "                if not book_elements: # No more editions on this page\n",
    "                    print(f\"No more editions on {page_url}\")\n",
    "                    break\n",
    "\n",
    "                for book in book_elements:\n",
    "                    # Extract title and format\n",
    "                    title_element = book.find(\"a\", class_=\"bookTitle\")\n",
    "                    title = title_element.text.strip() if title_element else \"N/A\"\n",
    "                    book_titles.append(title)\n",
    "\n",
    "                    # Extract publication date and publisher\n",
    "                    publication_info = book.find_all(\"div\", class_=\"dataRow\")\n",
    "                    if len(publication_info) > 1:\n",
    "                        pub_parts = publication_info[1].text.strip().split('by')\n",
    "                        publication_date = pub_parts[0].replace(\"Published\", \"\").strip() if len(pub_parts) > 0 else \"N/A\"\n",
    "                        publisher = pub_parts[1].strip() if len(pub_parts) > 1 else \"N/A\"\n",
    "                    else:\n",
    "                        publication_date = \"N/A\"\n",
    "                        publisher = \"N/A\"\n",
    "                    publication_dates.append(publication_date)\n",
    "                    publishers.append(publisher)\n",
    "\n",
    "                    # Extract format and page count\n",
    "                    if len(publication_info) > 2:\n",
    "                        format_parts = publication_info[2].text.strip().split(\", \")\n",
    "                        book_format = format_parts[0] if len(format_parts) > 0 else \"N/A\"\n",
    "                        page_count = format_parts[1].replace(\"pages\", \"\").strip() if len(format_parts) > 1 else \"N/A\"\n",
    "                    else:\n",
    "                        book_format = \"N/A\"\n",
    "                        page_count = \"N/A\"\n",
    "                    formats.append(book_format)\n",
    "                    page_counts.append(page_count)\n",
    "\n",
    "                    # Extract author name\n",
    "                    author_element = book.find(\"a\", class_=\"authorName\")\n",
    "                    author = author_element.text.strip() if author_element else \"N/A\"\n",
    "                    authors.append(author)\n",
    "\n",
    "                    # Extract author ID from the href\n",
    "                    if author_element and author_element.has_attr('href'):\n",
    "                        match = re.search(r'/author/show/(\\d+)', author_element['href'])\n",
    "                        author_id = match.group(1) if match else \"N/A\"\n",
    "                    else:\n",
    "                        author_id = \"N/A\"\n",
    "                    author_ids.append(author_id)\n",
    "\n",
    "                    # Extract ISBN\n",
    "                    isbn_element = book.find('div', class_='dataTitle', text=re.compile(r'ISBN:'))\n",
    "                    isbn = isbn_element.find_next(\"div\", class_=\"dataValue\").text.strip().split()[0] if isbn_element else \"N/A\"\n",
    "                    isbns.append(isbn)\n",
    "\n",
    "                    # Extract ASIN\n",
    "                    asin_element = book.find('div', class_='dataTitle', text=re.compile(r'ASIN:'))\n",
    "                    asin = asin_element.find_next(\"div\", class_=\"dataValue\").text.strip() if asin_element else \"N/A\"\n",
    "                    asins.append(asin)\n",
    "\n",
    "                    # Extract edition language\n",
    "                    language_element = book.find('div', class_='dataTitle', text=re.compile(r'Edition language:'))\n",
    "                    language = language_element.find_next(\"div\", class_=\"dataValue\").get_text(strip=True) if language_element else \"N/A\"\n",
    "                    languages.append(language)\n",
    "\n",
    "                    # Extract rating and rating count\n",
    "                    rating_element = book.find('div', class_='dataTitle', text=re.compile(r'Average rating:'))\n",
    "                    if rating_element:\n",
    "                        rating_info = rating_element.find_next(\"div\", class_=\"dataValue\").text.strip().split()\n",
    "                        rating = rating_info[0]\n",
    "                        rating_count = rating_info[1].replace(\"(\", \"\").replace(\"ratings)\", \"\").replace(\",\", \"\")\n",
    "                    else:\n",
    "                        rating = \"N/A\"\n",
    "                        rating_count = \"N/A\"\n",
    "                    ratings.append(rating)\n",
    "                    rating_counts.append(rating_count)\n",
    "\n",
    "                    # Extract book URL\n",
    "                    book_title_link = book.find('a', class_='bookTitle')\n",
    "                    if book_title_link and book_title_link.has_attr('href'):\n",
    "                        book_url = f\"https://www.goodreads.com{book_title_link['href']}\"\n",
    "                    else:\n",
    "                        book_url = \"N/A\"\n",
    "                    book_urls.append(book_url)\n",
    "\n",
    "                # Go to the next page of editions for this book (if existing)\n",
    "                page += 1\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to fetch {page_url} with status code {response.status_code}\")\n",
    "                break \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Skipping URL: {url}\")\n",
    "            break\n",
    "\n",
    "    # Save progress every save_every URLs\n",
    "    if (i + 1) % save_every == 0:\n",
    "        df_partial = pd.DataFrame({\n",
    "            \"Title\": book_titles,\n",
    "            \"Publication Date\": publication_dates,\n",
    "            \"Publisher\": publishers,\n",
    "            \"Format\": formats,\n",
    "            \"Page Count\": page_counts,\n",
    "            \"Author\": authors,\n",
    "            \"Author ID\": author_ids, \n",
    "            \"ISBN\": isbns,\n",
    "            \"ASIN\": asins,\n",
    "            \"Edition Language\": languages,\n",
    "            \"Average Rating\": ratings,\n",
    "            \"Rating Count\": rating_counts,\n",
    "            \"Book URL\": book_urls\n",
    "        })\n",
    "        df_partial.to_csv(output_file, index=False)\n",
    "        print(f\"Progress saved at URL {i+1}\")\n",
    "\n",
    "        # Final save\n",
    "df_5 = pd.DataFrame({\n",
    "    \"Title\": book_titles,\n",
    "    \"Publication Date\": publication_dates,\n",
    "    \"Publisher\": publishers,\n",
    "    \"Format\": formats,\n",
    "    \"Page Count\": page_counts,\n",
    "    \"Author\": authors,\n",
    "    \"Author ID\": author_ids,\n",
    "    \"ISBN\": isbns,\n",
    "    \"ASIN\": asins,\n",
    "    \"Edition Language\": languages,\n",
    "    \"Average Rating\": ratings,\n",
    "    \"Rating Count\": rating_counts,\n",
    "    \"Book URL\": book_urls\n",
    "})\n",
    "df_5.to_csv(\"editions_metadata_final_def.csv\", index=False)\n",
    "print(\"Scraping completed and saved to goodreads_editions_final.csv\")\n",
    "df_5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee292e00-75cf-4962-a4cb-da28c38bcefb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/8226: https://www.goodreads.com/work/editions/229064092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/vp8hdnn973qb6lcbrr7_4bx40000gn/T/ipykernel_1503/240379700.py:83: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  isbn_element = book.find('div', class_='dataTitle', text=re.compile(r'ISBN:'))\n",
      "/var/folders/hc/vp8hdnn973qb6lcbrr7_4bx40000gn/T/ipykernel_1503/240379700.py:88: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  asin_element = book.find('div', class_='dataTitle', text=re.compile(r'ASIN:'))\n",
      "/var/folders/hc/vp8hdnn973qb6lcbrr7_4bx40000gn/T/ipykernel_1503/240379700.py:93: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  language_element = book.find('div', class_='dataTitle', text=re.compile(r'Edition language:'))\n",
      "/var/folders/hc/vp8hdnn973qb6lcbrr7_4bx40000gn/T/ipykernel_1503/240379700.py:98: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  rating_element = book.find('div', class_='dataTitle', text=re.compile(r'Average rating:'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2/8226: https://www.goodreads.com/work/editions/246919373\n",
      "Processing 3/8226: https://www.goodreads.com/work/editions/227465077\n",
      "Processing 4/8226: https://www.goodreads.com/work/editions/85156807\n",
      "Processing 5/8226: https://www.goodreads.com/work/editions/91731825\n",
      "Processing 6/8226: https://www.goodreads.com/work/editions/94298597\n",
      "Processing 7/8226: https://www.goodreads.com/work/editions/3498000\n",
      "Processing 8/8226: https://www.goodreads.com/work/editions/2494662\n",
      "Processing 9/8226: https://www.goodreads.com/work/editions/153254462\n",
      "Processing 10/8226: https://www.goodreads.com/work/editions/67516039\n"
     ]
    }
   ],
   "source": [
    "def sleep():\n",
    "    sleep_time = 1 + random.random() # decrease it to 1 sec for faster scraping \n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = df['Generated_URL'].tolist()\n",
    "total = len(urls)\n",
    "\n",
    "# Initialize lists to store each field for the DataFrame\n",
    "book_titles = []\n",
    "publication_dates = []\n",
    "publishers = []\n",
    "formats = []\n",
    "page_counts = []\n",
    "authors = []\n",
    "author_ids = []\n",
    "isbns = []\n",
    "asins = []\n",
    "languages = []\n",
    "ratings = []\n",
    "rating_counts = []\n",
    "book_urls = []\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36'\n",
    "}\n",
    "\n",
    "save_every = 50  # Save progress every 50 URLs\n",
    "output_file = \"goodreads_editions_data_progress.csv\"\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    print(f\"Processing {i+1}/{total}: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        sleep()\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            book_elements = soup.find_all(\"div\", class_=\"elementList clearFix\")\n",
    "\n",
    "            for book in book_elements:\n",
    "                # Extract title and format\n",
    "                title_element = book.find(\"a\", class_=\"bookTitle\")\n",
    "                title = title_element.text.strip() if title_element else \"N/A\"\n",
    "                book_titles.append(title)\n",
    "\n",
    "                # Extract publication date and publisher\n",
    "                publication_info = book.find_all(\"div\", class_=\"dataRow\")\n",
    "                if len(publication_info) > 1:\n",
    "                    pub_parts = publication_info[1].text.strip().split('by')\n",
    "                    publication_date = pub_parts[0].replace(\"Published\", \"\").strip() if len(pub_parts) > 0 else \"N/A\"\n",
    "                    publisher = pub_parts[1].strip() if len(pub_parts) > 1 else \"N/A\"\n",
    "                else:\n",
    "                    publication_date = \"N/A\"\n",
    "                    publisher = \"N/A\"\n",
    "                publication_dates.append(publication_date)\n",
    "                publishers.append(publisher)\n",
    "\n",
    "                # Extract format and page count\n",
    "                if len(publication_info) > 2:\n",
    "                    format_parts = publication_info[2].text.strip().split(\", \")\n",
    "                    book_format = format_parts[0] if len(format_parts) > 0 else \"N/A\"\n",
    "                    page_count = format_parts[1].replace(\"pages\", \"\").strip() if len(format_parts) > 1 else \"N/A\"\n",
    "                else:\n",
    "                    book_format = \"N/A\"\n",
    "                    page_count = \"N/A\"\n",
    "                formats.append(book_format)\n",
    "                page_counts.append(page_count)\n",
    "\n",
    "                # Extract author name\n",
    "                author_element = book.find(\"a\", class_=\"authorName\")\n",
    "                author = author_element.text.strip() if author_element else \"N/A\"\n",
    "                authors.append(author)\n",
    "\n",
    "                # Extract author ID from the href\n",
    "                if author_element and author_element.has_attr('href'):\n",
    "                    match = re.search(r'/author/show/(\\d+)', author_element['href'])\n",
    "                    author_id = match.group(1) if match else \"N/A\"\n",
    "                else:\n",
    "                    author_id = \"N/A\"\n",
    "                author_ids.append(author_id)\n",
    "\n",
    "                # Extract ISBN\n",
    "                isbn_element = book.find('div', class_='dataTitle', text=re.compile(r'ISBN:'))\n",
    "                isbn = isbn_element.find_next(\"div\", class_=\"dataValue\").text.strip().split()[0] if isbn_element else \"N/A\"\n",
    "                isbns.append(isbn)\n",
    "\n",
    "                # Extract ASIN\n",
    "                asin_element = book.find('div', class_='dataTitle', text=re.compile(r'ASIN:'))\n",
    "                asin = asin_element.find_next(\"div\", class_=\"dataValue\").text.strip() if asin_element else \"N/A\"\n",
    "                asins.append(asin)\n",
    "\n",
    "                # Extract edition language\n",
    "                language_element = book.find('div', class_='dataTitle', text=re.compile(r'Edition language:'))\n",
    "                language = language_element.find_next(\"div\", class_=\"dataValue\").get_text(strip=True) if language_element else \"N/A\"\n",
    "                languages.append(language)\n",
    "\n",
    "                # Extract rating and rating count\n",
    "                rating_element = book.find('div', class_='dataTitle', text=re.compile(r'Average rating:'))\n",
    "                if rating_element:\n",
    "                    rating_info = rating_element.find_next(\"div\", class_=\"dataValue\").text.strip().split()\n",
    "                    rating = rating_info[0]\n",
    "                    rating_count = rating_info[1].replace(\"(\", \"\").replace(\"ratings)\", \"\").replace(\",\", \"\")\n",
    "                else:\n",
    "                    rating = \"N/A\"\n",
    "                    rating_count = \"N/A\"\n",
    "                ratings.append(rating)\n",
    "                rating_counts.append(rating_count)\n",
    "\n",
    "                # Extract book URL\n",
    "                book_title_link = book.find('a', class_='bookTitle')\n",
    "                if book_title_link and book_title_link.has_attr('href'):\n",
    "                    book_url = f\"https://www.goodreads.com{book_title_link['href']}\"\n",
    "                else:\n",
    "                    book_url = \"N/A\"\n",
    "                book_urls.append(book_url)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Skipping URL: {url}\")\n",
    "        continue\n",
    "\n",
    "    # Save progress every save_every URLs\n",
    "    if (i + 1) % save_every == 0:\n",
    "        df_partial = pd.DataFrame({\n",
    "            \"Title\": book_titles,\n",
    "            \"Publication Date\": publication_dates,\n",
    "            \"Publisher\": publishers,\n",
    "            \"Format\": formats,\n",
    "            \"Page Count\": page_counts,\n",
    "            \"Author\": authors,\n",
    "            \"Author ID\": author_ids, \n",
    "            \"ISBN\": isbns,\n",
    "            \"ASIN\": asins,\n",
    "            \"Edition Language\": languages,\n",
    "            \"Average Rating\": ratings,\n",
    "            \"Rating Count\": rating_counts,\n",
    "            \"Book URL\": book_urls\n",
    "        })\n",
    "        df_partial.to_csv(output_file, index=False)\n",
    "        print(f\"Progress saved at URL {i+1}\")\n",
    "\n",
    "# Final save\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": book_titles,\n",
    "    \"Publication Date\": publication_dates,\n",
    "    \"Publisher\": publishers,\n",
    "    \"Format\": formats,\n",
    "    \"Page Count\": page_counts,\n",
    "    \"Author\": authors,\n",
    "    \"Author ID\": author_ids,\n",
    "    \"ISBN\": isbns,\n",
    "    \"ASIN\": asins,\n",
    "    \"Edition Language\": languages,\n",
    "    \"Average Rating\": ratings,\n",
    "    \"Rating Count\": rating_counts,\n",
    "    \"Book URL\": book_urls\n",
    "})\n",
    "df.to_csv(\"goodreads_editions_metadata_final_def.csv\", index=False)\n",
    "print(\"Scraping completed and saved to goodreads_editions_final.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27cd1969-4d76-4dcf-baf0-dc9410a9934e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20382\n"
     ]
    }
   ],
   "source": [
    "file2 = 'goodreads_editions_metadata_final_def.csv'\n",
    "\n",
    "\n",
    "# Read the core dataset as a pandas DataFrame and show the Goodreads link column\n",
    "df = pd.read_csv(file2) # for testing a few lines, add [0:3]\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f90875e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"editions_metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
